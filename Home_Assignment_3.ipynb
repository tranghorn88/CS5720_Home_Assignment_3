{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tranghorn88/CS5720_Home_Assignment_3/blob/main/Home_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsZkzlWhE5yA",
        "outputId": "6fa3ad6d-3f7d-4b9a-9830-6fa2cb7986a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "Epoch 1/5\n",
            "172/172 [==============================] - 1428s 8s/step - loss: 2.5715\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 1354s 8s/step - loss: 1.8703\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 1367s 8s/step - loss: 1.6262\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 1387s 8s/step - loss: 1.4957\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 1353s 8s/step - loss: 1.4180\n",
            "\n",
            "--- Generated Text at Temperature = 0.2 ---\n",
            "To be, or not to be consul,\n",
            "And then the world the world of his head.\n",
            "\n",
            "KING EDWARD IV:\n",
            "What say you shall be to the hand of his hands.\n",
            "\n",
            "CAPULET:\n",
            "As it is the world he is a word of his son.\n",
            "\n",
            "CAPULET:\n",
            "A blood of this fair soul think you will have so many of the state,\n",
            "And then the last of the world of his father,\n",
            "And then the world of this son of strike,\n",
            "And when you have done the sun that have been so fair and man the house:\n",
            "And then the marriage of the world of his father.\n",
            "\n",
            "CORIOLANUS:\n",
            "I will to the part of the co\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Generated Text at Temperature = 0.8 ---\n",
            "To be, or not to be:\n",
            "\n",
            "First Senator:\n",
            "Which, he would have we did be real of\n",
            "'Twas straighted to the bastardness,\n",
            "Even this nothing and for this resolving eld,\n",
            "Which in commission like, how like a measure of my shame,\n",
            "Both in my warring, have warwake thee do their power?\n",
            "Did me for those wond, pray'd, by your father?\n",
            "\n",
            "AUFIDIUS:\n",
            "Where in me not sleak on, in heaven, which is dear indeed\n",
            "To all the wall, when you crow's?\n",
            "\n",
            "MENENIUS:\n",
            "He's best all ho to me, for her better more?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "She is suspoces and sly \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Generated Text at Temperature = 1.2 ---\n",
            "To be, or not to bed,\n",
            "Ah,\n",
            "Tongue them\n",
            "I plaised in't so, !\n",
            "Bothakine! all your eyes? No, be,'s him! I'ld hear athem?\n",
            "But him it shall be end;\n",
            "His smallsher, it be werewelf, I'll I bre brook!\n",
            "Remave\n",
            "estaint on her.\n",
            "\n",
            "BRENV:\n",
            "Consotmance:\n",
            "I did now, he is plouse: let's becaush Mercur'e.\n",
            "Gawnel's you unto this; what's I murds,\n",
            "Of no personing?\n",
            "\n",
            "MENENIUS:\n",
            "Say 'there kies her cause?\n",
            "\n",
            "CAMISBe tail? Our Romeo bride me! The TINCES:\n",
            "E:\n",
            "But, being corduct with, Citizens:\n",
            "You will can give it injureit with it it hath\n",
            "haight no\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "### Implementing an RNN for Text Generation\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "   \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Length of text: {len(text)} characters\")\n",
        "\n",
        "# Preprocess the text\n",
        "vocabulary = sorted(set(text))\n",
        "char_to_idx = {u: i for i, u in enumerate(vocabulary)}\n",
        "idx_to_char = np.array(vocabulary)\n",
        "text_as_int = np.array([char_to_idx[c] for c in text])\n",
        "\n",
        "# Create training examples\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Prepare batches\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Build the model\n",
        "vocabulary_size = len(vocabulary)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocabulary_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(batch_shape=(batch_size, None)),\n",
        "        tf.keras.layers.Embedding(vocabulary_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocabulary_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Compile and train the model\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "EPOCHS = 5\n",
        "model.fit(dataset, epochs=EPOCHS)\n",
        "\n",
        "# Generate text with temperature scaling\n",
        "def generate_text(model, start_string, temperature=1.0, num_generate=500):\n",
        "    input_eval = [char_to_idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    # Find the LSTM layer and reset its states\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.LSTM):\n",
        "            layer.reset_states()\n",
        "            break\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Rebuild model for generation (batch size = 1)\n",
        "gen_model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=1)\n",
        "gen_model.set_weights(model.get_weights())\n",
        "gen_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Generate and print outputs for different temperatures to observe randomness\n",
        "temperatures = [0.2, 0.8, 1.2]\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- Generated Text at Temperature = {temp} ---\")\n",
        "    generated_text = generate_text(gen_model, start_string=\"To be, or not to be\", temperature=temp)\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NLP Preprocessing Pipeline\n",
        "# Install and import NLTK\n",
        "!pip install -q nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_func(sentence):\n",
        "    # Tokenize using RegexpTokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    print(\"1. Original Tokens:\", tokens)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
        "    print(\"2. Tokens Without Stopwords:\", tokens_without_stopwords)\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens_without_stopwords]\n",
        "    print(\"3. Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Test the function with the sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "preprocess_func(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGNlT3kQdqM9",
        "outputId": "d9410d0e-a46f-4c8e-d832-326dd69890de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Original Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri']\n",
            "2. Tokens Without Stopwords: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri']\n",
            "3. Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Named Entity Recognition with SpaCy\n",
        "# Import library\n",
        "import spacy\n",
        "\n",
        "# Download the English language model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "# Process the sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print each detected entity's text, label, and start and end character positions\n",
        "print(\"Named Entities Detected:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text:30} - Label: {ent.label_:15} (Start Position: {ent.start_char} - End Position: {ent.end_char})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95j0DKCFi_7T",
        "outputId": "3552a639-0095-401e-a88e-036decb1945c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Named Entities Detected:\n",
            "\n",
            "Text: Barack Obama                   - Label: PERSON          (Start Position: 0 - End Position: 12)\n",
            "Text: 44th                           - Label: ORDINAL         (Start Position: 27 - End Position: 31)\n",
            "Text: the United States              - Label: GPE             (Start Position: 45 - End Position: 62)\n",
            "Text: the Nobel Peace Prize          - Label: WORK_OF_ART     (Start Position: 71 - End Position: 92)\n",
            "Text: 2009                           - Label: DATE            (Start Position: 96 - End Position: 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Scaled Dot-Product Attention\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "\n",
        "# Define the softmax function to normalize attention scores\n",
        "def softmax_func(x):\n",
        "    # Subtract max for numerical stability before applying exponentiation\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    # Devide by the sum to get softmax probabilities (row-wise)\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "# Define the scaled dot-product attention function\n",
        "def scaled_dot_product_attention_func(Q, K, V):\n",
        "    # Get the dimension of the key vectors\n",
        "    d_k = Q.shape[-1]\n",
        "    # Compute raw attention scores by dot product of Q and K transpose, then scale\n",
        "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
        "    # Apply softmax to get attention weights\n",
        "    att_weights = softmax_func(scores)\n",
        "    output = np.matmul(att_weights, V)\n",
        "    return att_weights, output\n",
        "\n",
        "# Test input matrices (Q, K, V)\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "# Run scaled dot-product attention mechanism\n",
        "weights, output = scaled_dot_product_attention_func(Q, K, V)\n",
        "\n",
        "# Display attention weights matrix (after softmax)\n",
        "print(\"Attention Weights:\\n\", weights)\n",
        "# Display the final weighted sum of values (context vector)\n",
        "print(\"Final Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-TxSJA_y04s",
        "outputId": "0679e35b-7a7b-4ec1-e478-e235a0626619"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "Final Output:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Analysis using HuggingFace Transformers\n",
        "# Import the HuggingFace pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained sentiment-analysis model\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Input sentence to analyze\n",
        "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# Run sentiment analysis\n",
        "result = classifier(sentence)[0]\n",
        "\n",
        "# Print sentiment label and confidence score\n",
        "print(\"Sentiment:\", result['label'])\n",
        "print(\"Confidence Score:\", round(result['score'], 4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN8oVs7A4346",
        "outputId": "b9bb427c-ce3e-4436-e65e-a981e0d7ac6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/+k53u6Pam+5gnDdLSZNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}